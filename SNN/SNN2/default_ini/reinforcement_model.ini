[reinforcement_weights_file]
type=reinforcementModel
value={ckpt_path}/reinfocement_net_weight_{appendix}_{uxtime_sec}.h5

[qoe_threshold]
type=reinforcementModel
value=80.0

[ReinforcementMetrics]
type=reinforcementModel
value=[]

[ReinforcementLosses]
type=reinforcementModel
value=['HuberActorCritic']

[ReinforcementLossParameters]
type=reinforcementModel
value=[]

[gamma]
type=reinforcementModel
value=0.99

[entropy_beta]
type=reinforcementModel
value=0.01

[ReinforcementAdamStrength]
type=reinforcementModel
value=0.00015

[ReinforcementAdamEpsilon]
type=reinforcementModel
value=1e-3

[ReinforcementAdamClipNorm]
type=reinforcementModel
value=0.1

[InterruptionPenalty]
type=reinforcementModel
value=-10

[max_steps]
type=reinforcementModel
value=10000

[training_epochs]
type=reinforcementModel
value=1

[reward_class_flag]
type=reinforcementModel
value=False

[RewardFunction]
type=reinforcementModel
value=originalReward

[perfEval_class_flag]
type=reinforcementModel
value=False

[PerfEval]
type=reinforcementModel
value=defaultRLPerfEval

[ObsPP_class_flag]
type=reinforcementModel
value=False

[ObservationPreProcessing]
type=reinforcementModel
value=cmExpansion

[ActionPolicy]
type=reinforcementModel
value=NNPolicy

[reinforce_actions]
type=reinforcementModel
value=2

[exploration_p]
type=reinforcementModel
value=0.0

[exploration_decay]
type=reinforcementModel
value=0.0

[train_batch_size]
type=reinforcementModel
value=1

[fresh_mem_only]
type=reinforcementModel
value=True

[stop_thresholds]
type=reinforcementModel
value=True

[action_interleave]
type=reinforcementModel
value=1

;[p0_correlation_threshold]
;type=reinforcementModel
;value=-0.90

;[p1_correlation_threshold]
;type=reinforcementModel
;value=0.90

;[margin_chainging_point_threshold]
;type=reinforcementModel
;value=0.0

;[exp_exploration_threshold]
;type=reinforcementModel
;value=6
